{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc4a58be",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c712610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd0f6b2b",
   "metadata": {},
   "source": [
    "## Export an ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bfc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything.modeling.image_encoder import ImageEncoderViT\n",
    "\n",
    "checkpoint = \"../savedmodel/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "# print(sam.image_encoder)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "image_encoder=ImageEncoderViT(\n",
    "    depth=32,\n",
    "    embed_dim=1280,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=16,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[7, 15, 23, 31],\n",
    "    window_size=14,\n",
    "    out_chans=256,\n",
    ")\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Load the weights from sam's image_encoder\n",
    "image_encoder.load_state_dict(sam.image_encoder.state_dict())\n",
    "image_encoder.eval()\n",
    "\n",
    "# save image_encoder model\n",
    "torch.save(image_encoder.state_dict(), \"../savedmodel/ImageEncoderViT.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5433e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "window_size 14\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load image_encoder model\n",
    "image_encoder=ImageEncoderViT(\n",
    "    depth=32,\n",
    "    embed_dim=1280,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=16,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[7, 15, 23, 31],\n",
    "    window_size=14,\n",
    "    out_chans=256,\n",
    ")\n",
    "image_encoder.load_state_dict(torch.load(\"../savedmodel/ImageEncoderViT.pth\"))\n",
    "image_encoder.eval()\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "example_input = {\"input\": torch.randn(1, 3, 1024, 1024)}  # adjust as necessary\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"point_coords\": {1: \"num_points\"},\n",
    "    \"point_labels\": {1: \"num_points\"},\n",
    "}\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(\"../savedmodel/ImageEncoderViT.onnx\", \"wb\") as f:\n",
    "        torch.onnx.export(\n",
    "            image_encoder,\n",
    "            example_input.values(),\n",
    "            \"../savedmodel/ImageEncoderViT.onnx\",\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,\n",
    "            input_names=example_input.keys(),\n",
    "            output_names=[\"output\"],\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a6f11bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/blocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.1/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.2/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.2/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.3/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.3/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.4/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.4/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.5/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.5/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.6/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.6/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.7/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.7/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.8/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.8/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.9/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.9/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.10/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.10/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.11/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.11/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.12/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.12/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.13/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.13/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.14/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.14/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.15/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.15/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.16/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.16/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.17/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.17/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.18/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.18/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.19/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.19/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.20/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.20/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.21/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.21/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.22/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.22/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.23/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.23/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.24/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.24/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.25/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.25/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.26/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.26/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.27/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.27/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.28/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.28/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.29/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.29/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.30/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.30/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks.31/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks.31/attn/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "# quantize\n",
    "encode_qt_path = \"../savedmodel/ImageEncoderViT_qt.onnx\"\n",
    "quantize_dynamic(\"../savedmodel/ImageEncoderViT.onnx\", encode_qt_path, weight_type=QuantType.QUInt8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
